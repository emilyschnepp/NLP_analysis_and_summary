{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JkGfCkFVVFb",
        "outputId": "2e3a4155-8839-4116-db91-c15a82d8850f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5a6oWXnVmM2",
        "outputId": "b06d08f1-0b0f-498f-e924-69dce95a8ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kHb1TjC7VcHn",
        "outputId": "3a0bb47a-c1b3-4e54-e648-e16fee84d878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "id": "rpEpJr9hVdfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing required libraries.\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "from docx import Document\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Defining file paths\n",
        "filePaths = ['/content/P2025Chapter1.docx', '/content/P2025Chapter2.docx', '/content/P2025Chapter3.docx', '/content/P2025Chapter4.docx', '/content/P2025Chapter5.docx',\n",
        "             '/content/P2025Chapter6.docx', '/content/P2025Chapter7.docx', '/content/P2025Chapter8.docx', '/content/P2025Chapter9.docx', '/content/P2025Chapter10.docx',\n",
        "             '/content/P2025Chapter11.docx', '/content/P2025Chapter12.docx', '/content/P2025Chapter13.docx', '/content/P2025Chapter14.docx', '/content/P2025Chapter15.docx',\n",
        "             '/content/P2025Chapter16.docx', '/content/P2025Chapter17.docx', '/content/P2025Chapter18.docx', '/content/P2025Chapter19.docx', '/content/P2025Chapter20.docx',\n",
        "             '/content/P2025Chapter21.docx', '/content/P2025Chapter22.docx', '/content/P2025Chapter23.docx', '/content/P2025Chapter24.docx', '/content/P2025Chapter25.docx',\n",
        "             '/content/P2025Chapter26.docx', '/content/P2025Chapter27.docx', '/content/P2025Chapter28.docx', '/content/P2025Chapter29.docx', '/content/P2025Chapter30.docx',\n",
        "             '/content/P2025Foreword.docx', '/content/P2025Onward.docx', '/content/P2025Section1.docx', '/content/P2025Section2.docx', '/content/P2025Section3.docx',\n",
        "             '/content/P2025Section4.docx', '/content/P2025Section5.docx']\n",
        "\n",
        "#Defining a function to extract the text from the docx file.\n",
        "def docxTextExtraction(docx_file):\n",
        "  doc = Document(docx_file)\n",
        "  return \"\\n\".join([para.text\n",
        "                    for para in doc.paragraphs])\n",
        "\n",
        "documents = []\n",
        "\n",
        "for path in filePaths:\n",
        "    try:\n",
        "        text = docxTextExtraction(path)\n",
        "        documents.append(text)\n",
        "        print(f\"Loaded {path} successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {path}: {str(e)}\")\n",
        "\n",
        "    #Defining functions to preprocess the corpus and divide the text into chunks.\n",
        "    def preprocessCorp(text):\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.replace(\"Mandate for Leadership: The Conservative Promise\", \"\")\n",
        "        text = text.replace(\"Mandate for Leadership\", \"\")\n",
        "        return text\n",
        "    #Chunking texts to handle large documents that exceed BART's token limits.\n",
        "    def textChunks(text, max_tokens=200):\n",
        "        sentences = sent_tokenize(text)\n",
        "        chunks = []\n",
        "        currentChunk = []\n",
        "        currentLength = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentenceLength = len(nltk.word_tokenize(sentence))\n",
        "            if currentLength + sentenceLength <= max_tokens:\n",
        "                currentChunk.append(sentence)\n",
        "                currentLength += sentenceLength\n",
        "            else:\n",
        "                chunks.append(\" \".join(currentChunk))\n",
        "                currentChunk = [sentence]\n",
        "                currentLength = sentenceLength\n",
        "            if currentChunk:\n",
        "              chunks.append(\" \".join(currentChunk))\n",
        "        return chunks\n",
        "    #Defining functions for summarization using BART\n",
        "    def summarizeWithBart(text, max_length=100, min_length=50):\n",
        "        tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "        inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        summaryIDs = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        summary = tokenizer.decode(summaryIDs[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "    def summarizeWithBartLarge(text, max_tokens=200, max_length=100, min_length=50, max_summary_length_ratio=0.2):\n",
        "        chunks = textChunks(text, max_tokens=max_tokens)\n",
        "        totalLength = sum(len(nltk.word_tokenize(chunk)) for chunk in chunks)\n",
        "        targetLength = int(totalLength * max_summary_length_ratio)\n",
        "\n",
        "        summaries = []\n",
        "        currentSummaryLength = 0\n",
        "        for chunk in chunks:\n",
        "            summary = summarizeWithBart(chunk, max_length=max_length, min_length=min_length)\n",
        "            summaryLength = len(nltk.word_tokenize(summary))\n",
        "            if currentSummaryLength + summaryLength > targetLength:\n",
        "                break\n",
        "            summaries.append(summary)\n",
        "            currentSummaryLength += summaryLength\n",
        "\n",
        "        return \" \".join(summaries)\n",
        "    #Defining a function to reconstruct paragraphs with LDA topic modeling.\n",
        "    #LDA provides topical clusters for better readability.\n",
        "    def assembleParagraphs(summary, numTopics='auto'):\n",
        "        sentences = sent_tokenize(summary)\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "        if numTopics == 'auto':\n",
        "            numTopics = min(10, len(sentences) // 5)\n",
        "\n",
        "        lda = LatentDirichletAllocation(n_components=numTopics, random_state=0)\n",
        "        lda.fit(X)\n",
        "        topics = lda.transform(X)\n",
        "\n",
        "        clusters = np.argmax(topics, axis=1)\n",
        "        clusteredSentences = [[] for _ in range(numTopics)]\n",
        "\n",
        "        for sentence, cluster in zip(sentences, clusters):\n",
        "            clusteredSentences[cluster].append(sentence)\n",
        "\n",
        "        structuredSummary = \"\\n\\n\".join([\" \".join(cluster) for cluster in clusteredSentences])\n",
        "        return structuredSummary\n",
        "\n",
        "    #Analyzing each part of the document.\n",
        "    for i, doc in enumerate(documents):\n",
        "        doc = preprocessCorp(doc)\n",
        "\n",
        "        summary = summarizeWithBartLarge(doc, max_tokens=200, max_length=100, min_length=50)\n",
        "        structuredSummary = assembleParagraphs(summary, numTopics='auto')\n",
        "        print(f\"Document {i+1} - Summary: {structuredSummary}\")"
      ],
      "metadata": {
        "id": "V7_WOo2ZViBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}