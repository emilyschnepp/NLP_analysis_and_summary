{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-WC1Dajv0YtX"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "id": "HvUD2sMK0Z_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textstat"
      ],
      "metadata": {
        "id": "JYV5PeOa0wdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "id": "Zdhc7cJM0x1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing required libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords, brown\n",
        "from docx import Document\n",
        "from wordcloud import WordCloud\n",
        "import textstat\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import seaborn as sns\n",
        "from math import pi\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('brown')\n",
        "\n",
        "# Download spaCy model\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# Define file paths\n",
        "filePaths = ['/content/P2025Chapter1.docx','/content/P2025Chapter2.docx','/content/P2025Chapter3.docx', '/content/P2025Chapter4.docx','/content/P2025Chapter5.docx','/content/P2025Chapter6.docx',\n",
        "             '/content/P2025Chapter7.docx','/content/P2025Chapter8.docx','/content/P2025Chapter9.docx', '/content/P2025Chapter10.docx', '/content/P2025Chapter11.docx','/content/P2025Chapter12.docx',\n",
        "             '/content/P2025Chapter13.docx', '/content/P2025Chapter14.docx', '/content/P2025Chapter15.docx', '/content/P2025Chapter16.docx', '/content/P2025Chapter17.docx','/content/P2025Chapter18.docx',\n",
        "             '/content/P2025Chapter19.docx','/content/P2025Chapter20.docx','/content/P2025Chapter21.docx', '/content/P2025Chapter22.docx','/content/P2025Chapter23.docx','/content/P2025Chapter24.docx',\n",
        "             '/content/P2025Chapter25.docx','/content/P2025Chapter26.docx','/content/P2025Chapter27.docx', '/content/P2025Chapter28.docx','/content/P2025Chapter29.docx','/content/P2025Chapter30.docx',\n",
        "             '/content/P2025Foreword.docx','/content/P2025Onward.docx','/content/P2025Section1.docx', '/content/P2025Section2.docx','/content/P2025Section3.docx','/content/P2025Section4.docx', '/content/P2025Section5.docx']\n",
        "\n",
        "#Defining a function to extract text from a .docx file.\n",
        "def extractDocXText(docx_file):\n",
        "    doc = Document(docx_file)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "documents = []\n",
        "names = []\n",
        "\n",
        "for path in filePaths:\n",
        "    try:\n",
        "        text = extractDocXText(path)\n",
        "        documents.append(text)\n",
        "        names.append(path.split('/')[-1].replace('.docx', ''))\n",
        "        print(f\"Loaded {path} successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {path}: {str(e)}\")\n",
        "\n",
        "    #Combining all parts to use the whole document for analysis.\n",
        "    wholeDocument = \" \".join(documents)\n",
        "\n",
        "    # Defining a function to preprocess the text by removing whitespace and words that often don't add context.\n",
        "    def preprocessCorp(text):\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.replace(\"Mandate for Leadership: The Conservative Promise\", \"\")\n",
        "        text = text.replace(\"Mandate for Leadership\", \"\")\n",
        "        return text\n",
        "    #Defining a function to calculate word and sentence statistics.\n",
        "    def calcStats(text):\n",
        "        words = nltk.word_tokenize(text)\n",
        "        sentences = sent_tokenize(text)\n",
        "        numWords = len(words)\n",
        "        numSentences = len(sentences)\n",
        "        avgSentenceLength = numWords / numSentences if numSentences != 0 else 0\n",
        "        return numWords, numSentences, avgSentenceLength\n",
        "    #Defining a function to calculate readability metrics.\n",
        "    def calcReadability(text):\n",
        "        return {\n",
        "            \"Syllable Count\": textstat.syllable_count(text),\n",
        "            \"Flesch Reading Ease\": textstat.flesch_reading_ease(text),\n",
        "            \"Flesch-Kincaid Grade\": textstat.flesch_kincaid_grade(text),\n",
        "            \"Gunning Fog\": textstat.gunning_fog(text),\n",
        "            \"SMOG Index\": textstat.smog_index(text),\n",
        "            \"Coleman-Liau Index\": textstat.coleman_liau_index(text),\n",
        "            \"Automated Readability Index\": textstat.automated_readability_index(text),\n",
        "            \"Dale-Chall Readability Score\": textstat.dale_chall_readability_score(text),\n",
        "            \"Difficult Words\": textstat.difficult_words(text)\n",
        "        }\n",
        "\n",
        "    #Creating a dataframe for calculation storage.\n",
        "    calcsDf = pd.DataFrame(columns=['Name', 'Words', 'Sentences', 'Avg Sentence Length',\n",
        "                               'Syllable Count', 'Flesch Reading Ease', 'Flesch-Kincaid Grade',\n",
        "                               'Gunning Fog', 'SMOG Index', 'Coleman-Liau Index',\n",
        "                               'Automated Readability Index', 'Dale-Chall Readability Score',\n",
        "                               'Difficult Words'])\n",
        "\n",
        "    #Looping through each document to calculate statistics and readability metrics.\n",
        "    for i, doc in enumerate(documents):\n",
        "        doc = preprocessCorp(doc)\n",
        "\n",
        "        numWords, numSentences, avgSentenceLength = calcStats(doc)\n",
        "        readabilityMetrics = calcReadability(doc)\n",
        "\n",
        "        #Adding the metrics to the dataframe.\n",
        "        calcsDf.loc[i] = [names[i], numWords, numSentences, avgSentenceLength] + list(readabilityMetrics.values())\n",
        "\n",
        "#Categorizing documents by Chapter, Section, or Other.\n",
        "calcsDf['Category'] = calcsDf['Name'].apply(lambda x: 'Chapter' if 'Chapter' in x else ('Section' if 'Section' in x else 'Other'))\n",
        "\n",
        "#Creating a radar chart to visualize readability metrics.\n",
        "#Defining a function to create a grayscale radar chart.\n",
        "def radarChart(df_row, title):\n",
        "    categories = list(df_row.index)\n",
        "    values = df_row.values.flatten().tolist()\n",
        "    values += values[:1]\n",
        "    angles = [n / float(len(categories)) * 2 * pi for n in range(len(categories))]\n",
        "    angles += angles[:1]\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "    ax.plot(angles, values, linewidth=2, linestyle='solid', color='black')\n",
        "    ax.fill(angles, values, 'grey', alpha=0.3)\n",
        "    plt.xticks(angles[:-1], categories, color='black', size=8)\n",
        "    plt.title(title, color='black')\n",
        "    plt.show()\n",
        "\n",
        "#Aggregating the readability metrics for the entire book and creating a radar chart.\n",
        "bookAgg = calcsDf[['Flesch Reading Ease', 'Flesch-Kincaid Grade', 'Gunning Fog', 'SMOG Index',\n",
        "                   'Coleman-Liau Index', 'Automated Readability Index', 'Dale-Chall Readability Score']].mean()\n",
        "radarChart(bookAgg, 'Readability Metrics - Whole Book')\n",
        "\n",
        "#Creating a line chart for readability\n",
        "#Creating a line chart by Category to visualize Flesch-Kincaid Grade using Grayscale\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.lineplot(x='Name', y='Flesch-Kincaid Grade', hue='Category', data=calcsDf, palette='Greys', marker='o')\n",
        "plt.title('Flesch-Kincaid Grade Across Chapters and Sections')\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Chapter/Section')\n",
        "plt.ylabel('Flesch-Kincaid Grade')\n",
        "plt.show()\n",
        "\n",
        "#Creating a word cloud of the most used difficult words in the entire book.\n",
        "brownUCWords = brown.words()\n",
        "freqDistBrown = nltk.FreqDist(brownUCWords)\n",
        "#Defining a function to handle wordcloud specific preprocessing steps.\n",
        "def preprocessForWordCloud(text):\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    words = [word.lower() for word in nltk.word_tokenize(text) if word.isalpha() and word.lower() not in stopWords]\n",
        "    words = list(set(words))\n",
        "    return \" \".join(words)\n",
        "\n",
        "#Defining a function to find difficult words based on syllable count and rarity (using Brown University corpus).\n",
        "def findDifficultWords(text, min_syllables=4, min_frequency=10):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    difficultWords = [word for word in words if textstat.syllable_count(word) >= min_syllables]\n",
        "    difficultWords = [word for word in difficultWords if freqDistBrown[word.lower()] <= min_frequency]\n",
        "    return difficultWords\n",
        "\n",
        "#Applying previously created functions (preprocessForWordcloud and findDifficultWords)\n",
        "preprocessedText = preprocessForWordCloud(wholeDocument)\n",
        "difficultWords = findDifficultWords(preprocessedText)\n",
        "\n",
        "#Generating the wordcloud.\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Greys').generate(\" \".join(difficultWords))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Difficult Words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3jk1gHLC5ObM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}